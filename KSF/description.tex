% This file is part of the GetRichOrDieTrying / KSF project
% Copyright 2019 the authors.

% to-do
% -----
% - deal with all CITEs
% - deal with all HOGG and KSF

\documentclass[12pt, fullpage, letterpaper]{article}
\usepackage{fancyheadings}
\usepackage{xspace}
\input{hogg_nsf}

% headers and footers
\setlength{\headsep}{2ex}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\newcommand{\shorttitle}{CDS\&E \ldots precision and discovery in cosmology experiments}
\lhead{\textcolor{darkgrey}{\textsf{Hogg \& Storey-Fisher / \shorttitle}}}
\rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
\cfoot{}
\newcommand{\KSF}[1]{}
\newcommand{\HOGG}[1]{}
%\newcommand{\KSF}[1]{\textcolor{teal}{KSF says: #1}}
%\newcommand{\HOGG}[1]{\textcolor{orange}{HOGG says: #1}}
\newcommand{\cf}{2pcf\xspace}
\newcommand{\LS}{\acronym{LS}\xspace}
\newcommand{\inv}{^{-1}}
\newcommand{\T}{^{\mathsf{T}}}

\begin{document}

The cold-dark-matter-with-a-cosmological-constant (\LCDM) model \citep{Blumenthal1984} is
very well established observationally, having
explained many different
kinds of data with a small number of free parameters (\citealt{LopezCorredoira2017} and references therein).
It has made significant ab-initio predictions (like the baryon acoustic feature; \citealt{EisensteinHu1998})
prior to the observations.
However, it's almost certainly \emph{not the true model}.
It seems unlikely that the dark sector contains no
non-gravitational interactions, and no interesting dynamical features (e.g. \citealt{Billyard2000}, \citealt{Gonzalez2018}).
What is well established is the following:
Whatever \emph{is} the true model \emph{is very well approximated} by
\LCDM\ over a wide range of scales in length and density.
The \LCDM\ model, then, deserves to be worked out, tested, and scrutinized at the
limits of what's possible (or at least sensible!).

In addition to the success of the \LCDM\ model---or
really because of it---the principal cosmological parameters have been
measured in data with percent-level precision \citep{Planck2018}.
These parameters can be thought of as the age, the densities in
various components, and the power spectrum of the initial conditions.
However, our percent-precision measurements have not been good enough to
detect potential deviations from \LCDM\, or to find new
cosmological and astrophysical effects.
The current and next generations of surveys hope to
make measurements of effects which enter at sub-percent levels (e.g. \citealt{Oyama2016}, \citealt{Rabold2017}, \citealt{Liao2017}),
which may lead to new physical insights.

\textbf{Here we propose to design, build, and exercise tools for the
analysis of large-scale structure that will
  be valuable for answering very precise cosmological questions}, even
as the questions themselves evolve and expand.
Given the success and precision of the current
cosmological model, new projects either have to obtain
\textbf{substantially more precision}, or else look for subtle
\textbf{departures from predictions}.
That is, we want to see small effects, and in particular, small
effects that might call into question the core (or simplifying) assumptions of the
\LCDM\ model.

Measuring things better than they have ever been measured before will require work along
multiple axes.
The first axis is \emph{volume}: We need to observe more spatial volume, and with new kinds
of tracers.
This is happening now, with projects like the \eBOSS\ survey within the \project{Sloan Digital Sky
  Survey \acronym{IV}} (\SDSSIV) surveying the largest volume to date
\citep{Dawson2015}.
Upcoming wide-field surveys include \DESI\ 
\citep{Aghamousa2016}, which starts observations in 2020, followed
by \NASA\ \project{Euclid} \citep{Amiaux2012} and
\LSST\ \citep{Ivezic2018} in 2021, and \NASA\ \project{\acronym{WFIRST}} in the mid-2020s
\citep{Akeson2019}.
These will observe 10 to 20 times more galaxies than current surveys---but this comes at a cost. As we look to larger
distances with denser tracers, we push our
observations to noisier targets with less-well-measured individual
properties, as well as more elaborate window functions and hard-to-model foregrounds.
That is, as the data get better in a global sense, it gets worse in an individual-object sense.

This leads to the second axis along which we have to work, which
is \emph{calibration, contamination, and window functions}.
As the data get harder to use at the individual-object level, it is
harder to know that we have surveyed the volume uniformly, or,
that we understand the non-uniformity of our sampling (e.g. \citealt{Sato2013}).
Survey window function knowledge starts to depend on things about the
observational projects that we can't know well, like small-scale
variations of the point-spread function or temporal variability of the
photometric bandpasses.
In this context we are pushed to data-driven techniques for
understanding the selection.

The third axis is \emph{statistical sophistication}.
As we simultaneously move to higher precision requirements, larger
volumes of data, and lower-precision individual-object data, it is 
easier and easier to get fooled.
This fooling can come in the form of systematic effects (like
photometric calibration or foregrounds) in the data projecting onto
tiny physical effects (like neutrino masses) in the clustering.
Alternatively it can come in the form of anomalies appearing: If the volume is
huge, the effective size of any search is huge.
We \emph{will} discover anomalies; the hard question will be whether
they are statistically significant or worth further study.
This gets into issues of multiple hypothesis testing, which we will
discuss below.
But these two issues---systematic effects and multiple hypothesis
testing---directly combine, because unmodeled or incorrectly modeled
systematic issues in the data will project onto discoverable
anomalies.
If we are going to search the next generation of surveys for
interesting new departures from baseline theoretical expectations,
we need a very solid statistical plan.

This proposal is to build and exercise general-purpose tools that will
make the next generation of (galaxy, quasar, and intensity-mapping)
large-scale structure (\LSS) surveys more precise, more powerful, and
more capable of making truly new discoveries.
The project has three themes, which connect to the three axes above in
various ways.
In the first theme, we replace the standard estimator for clustering or
correlation functions with something far more flexible, which permits us
to do more with more cosmological volume at fixed computational cost.
In the second, we analyze, attack, and re-build the self-calibration 
methods for handling systematics and window-function estimation.
In the third, we perform a very false-positive-safe search for anomalies
in the large-scale structure, to identify departures that might point to
new physics.
These three themes work together to create new measurement and discovery space,
increasing the scientific reach of the big investments we (as a community)
are making in large-scale structure.

\section{First theme: A new estimator for galaxy clustering}

The project will build and operate a new estimator for
galaxy clustering that obviates binning of objects (galaxies or
quasars or pixels) or pairs. Instead, it estimates continuous
functional forms for clustering as a function of scale, galaxy mass,
color, and so on.
These functions make a better representation of the
clustering than any binning; therefore this estimator can do more with
fewer clustering-model components.
This in turn lets it make more powerful measurements with less data,
and with less computational cost for uncertainty
propagation.

\subsection{The Two-Point Correlation Function}

Galaxy clustering is one of the most important tracers of the large-scale structure of the Universe.
The distribution of galaxies contains information about the Universe's evolution and fundamental components, as well as the evolution of the galaxies themselves \citep{Hamilton1988}.
To study the clustering of galaxies, we use observational galaxy surveys that range from hundreds to millions, and soon billions, of galaxies, and are pushing to even higher redshifts.
With the increased size and complexity of next-generation galaxy surveys, the tools currently used to characterize galaxy clustering will not be sufficient.

The distribution of galaxies in a volume of sky can be modeled as a spatial point process.
We can use the statistics of point processes to characterize their clustering, giving us insight into the physical processes that produce this distribution.
One of the most common statistics used to analyze galaxy surveys is the two-point correlation function (\cf). 
\textbf{The \cf is critical for inferring cosmological parameters, determining the distance scale in the early Universe, and constraining galaxy formation by examining how clustering depends on galaxy properties.}
 
The canonical estimator for the galaxy \cf, originally proposed by \cite{LandySzalay1993} (\LS), estimates the excess number of galaxies in bins of spatial separation $r$, compared to a statistically random distribution.
Given a set of $K$ bins, the \LS estimator counts the number of galaxy pairs $DD$ with a separation in each bin, and counts ``random'' pairs $RR$ from a uniformly distributed catalog  to correct for the survey boundary, as well as cross-pairs $DR$.
The estimator is then defined as $\xi(r) = [DD(r) - 2DR(r) + RR(r)]/RR(r)$,
where the terms are normalized vectors of pair counts in each bin, each having length $K$.

While the \LS estimator has better overall variance properties than the naive clustering estimator, it still suffers from several limitations. 
First, choosing the number of bins requires a trade-off between bias and variance (e.g. \citealt{Bailoni2016}).
Various works have addressed these issues by improving the random pair counting \citep{Demina2016}, optimizing the combination of pair counts \citep{VargasMagana2013}, and introducing a maximum likelihood approach \citep{BaxterRozo2013}, but this trade-off remains as it is inherent to binning.
Furthermore, for any finite binning in separation, this estimator contains a statistical bias when applied to highly clustered data (e.g. \citealt{Kerscher1998}). The \LS estimator also can't account for properties besides spatial separation.
Another binned dimension is often used to characterize the dependence
on galaxy properties: the galaxies are divided into subsamples to
analyze the relationship between their properties and clustering.
One tool developed to address this is the marked correlation function \citep{WhitePadmanabhan2009}, in which galaxies are weighted by the desired property (e.g. luminosity), but this approach still requires $r$-binning and doesn't account for the survey window.
While these estimators have been sufficient for past analyses, an improved estimator is necessary for future surveys.

\subsection{The Continuous-Function Estimator}

In order to avoid the issues introduced by finite binning with current estimators, we have developed an approach based on continuous basis functions. 
Inspired by linear least-squares fitting, we replace the scalar pair counts of \LS with a vector of the values output by a chosen basis function, and the normalization term by a tensor of these values.
We generalize the \LS estimator above to any set of $K$ basis functions $f$ of the pair, obtaining
\begin{eqnarray}\displaystyle
DD \equiv \sum_{n n'} f(T_n, T_{n'}) &\quad&
DR \equiv \sum_{n m} f(T_n, T_{m}) \nonumber\\
RR \equiv \sum_{m m'} f(T_m, T_{m'}) &\quad&
QQ \equiv \sum_{m m'} f(T_m, T_{m'}) \cdot f\T(T_m, T_{m'}),
\end{eqnarray}
where $DD$, $DR$, and $RR$ are vectors of length $K$, $T$ refers to the data (position and other information) for the given tracer, and $QQ$ is a $K$-by-$K$ matrix defined as the outer product of basis function evaluations of the random--random pairs.
The matrix $QQ$ encodes the non-orthonormality of the basis functions and is used to correct the projections, as follows:
From these statistics, we compute the \cf by
\begin{eqnarray}\displaystyle
\xi(T_l, T_{l'}) &\equiv& [QQ\inv \cdot (DD + 2DR - RR)]\T \cdot f(T_l, T_{l'}) \quad ,
\end{eqnarray}
where $T_l$ and $T_{l'}$ contain the data values at which to evaluate $\xi$.
Thus our estimator allows $\xi$ to be evaluated at \emph{any} separation, as well as at any other value of the data property used in the basis functions.
We have also shown that it is invariant under affine transformations of the basis (rotations and shears in the $f$ space).

This generalized two-point estimator removes the need for binning, as the basis functions can be continuous.
For the case of tophat (rectangular) basis functions, this reduces to the standard \LS\ estimator.
A set of basis functions can be chosen specific to the scales, features and properties of interest.
For example, higher order spline basis functions could be chosen to achieve a smooth statistic. 
A set of functions that select expected features in the \cf, such as the baryon acoustic feature, would allow for a more direct measurement of that feature.
Additionally, basis functions can depend on other properties of the tracer, such as galaxy luminosity or redshift.
Thus our estimator not only removes the need for binning in spatial separation, but can also be used to obviate binning or cuts where subsample approaches are currently needed.
This property of the estimator will come up below in the third theme (anomaly search).

We have implemented an initial version of the continuous-function correlation function estimator, within the \texttt{corrfunc} package \citep{Sinha2017}, which performs fast pair-counting on a lattice.
To test the estimator, we have generated lognormal catalogs with a known correlation functions. 
We will run our estimator on these mocks and expect these to show that, compared to the \LS\
  estimator, the continuous-function estimator produces an estimate with lower bias and
  lower variance with respect to the true \cf.
Given the enforced smoothness, this estimator should be able to not only produce a more accurate correlation function, but do so with fewer basis functions.
As the error on the inverse covariance matrix scales with the ratio of some power of the number of bins to the number of mock catalogs, our estimator can produce covariance matrices with fewer mock catalogs \citep{Hartlap2007}.
\textbf{As the cost of cosmological simulations required to build a set of mock catalogs is one of the limiting factors in \cf analyses; our estimator will significantly reduce this cost by containing more information in fewer basis functions.}

Alongside this work, we have revisited the point-process literature (at the intersection of mathematics and statistics) and identified how the biases from binning and edge correction are entering. 
We will use this understanding to develop an estimator that doesn't suffer from this bias.
We will combine these two approaches to develop and implement an estimator for galaxy clustering that is fundamentally unbiased and utilizes general basis functions.

\subsection{Applications: The BAO Peak and Galaxy Evolution}

The clustering of galaxies is a critical probe of the baryon acoustic oscillation (BAO) feature. 
This is a matter overdensity caused by acoustic waves of photon-baryon fluid traveling through the early Universe. 
The feature should appear at scale of around 120 Mpc/h today, and indeed we see a peak there in the \cf \citep{Eisenstein2005}. 
The exact BAO scale can be extracted from the binned \cf by comparing it to theoretical models (e.g. \citealt{Anderson2014}). 
With our estimator, we can directly project the galaxy pairs onto these models. 
Specifically, we will use as one of our basis functions a canonical model of the \cf with the Planck best-fit cosmological parameters. 
The other basis functions are derivatives of this model with respect to the parameters we are interested in; we can then update our base model and iterate until we converge on the best-fit model.
\textbf{This eliminates potential error due to the arbitrary choice of bins, and allows for a direct estimation of the cosmological parameters and the BAO scale.}
We will compare this method to the traditional technique, and expect to show that this method produces more precise measurements of the sound horizon with fewer components.

Another application is to galaxy evolution, one of the biggest open research topics in modern astrophysics.
The \cf allows us to ask questions about how the formation of galaxies is related to the evolution of structure in the Universe.
The dependence of galaxy clustering on the properties of galaxies, such as their line luminosity and star formation rate, gives insight into this relation \citep{Zehavi2004}.
Our estimator will allow us to extract even more information about this dependence. 
We plan to construct basis functions that contain a dependence on the galaxy luminosity, allowing for a combined encoding of both separation and luminosity. 
We can then quickly evaluate the estimator at any set of luminosities and produce a continuous \cf for each. 
We note that, while continuous, the estimator does not smooth out features in the way that kernel-based estimators do, but preserves and possibly enhances these features.
We will compare this method to the common subsampling approach; \textbf{we expect to show that this method will allow us to interpret the luminosity dependence in more detail and with higher precision.}

\section{Second theme: Adversarial approaches to systematics}

The project will build adversarial simulated data that is
designed to defeat current methodologies for finding and correcting
systematic effects of calibration and target selection in \LSS\ 
surveys. 
In tandem, we will build far more general calibration
programs, based on non-parametric models, that can detect these complex systematics and correct for them without

\subsection{Background: Systematics correction in galaxy surveys}

To make accurate cosmological measurements and detect potential deviations from the model, we require clean, unbiased datasets.
Observed galaxy catalogs are necessarily imperfect due to systematic effects in observation and instrumentation. 
In order to characterize the clustering from these surveys, we must model, fit, and remove these effects (e.g. \citealt{Agarwal2014}).

Systematics are any issues with the data that produce variations in the density or redshift distribution of the sample, therefore imprinting non-cosmological signals on large-scale structure analyses.
For instance, the galaxy number density varies with the width of the point spread function (PSF), as observations with a wider PSF may confuse star--galaxy separation.
Even small systematic effects, such as those from airmass and changing or variable photometric bandpasses, can bias large surveys. 
Objects in high-redshift surveys may be particularly sensitive to issues like color selection and instrument calibration.
Current methods assume simple linear systematics models that don't capture nonlinearities in these effects.
While linear corrections are adequate for most of today's surveys, next-generation surveys will have a combination of more complicated window functions, noisier foregrounds, and complex observational and instrumental effects.
Failure to fully correct for systematics in future analyses will propagate through clustering analyses and give biased results.

One common approach to correcting survey systematics is with regression-based techniques (see e.g. \citealt{Ross2010}).
Weights for important systematic parameters are determined by performing a fit, typically linear or quadratic, to adjust for dependencies between galaxy density and each parameter.
These are combined into a single weight for each galaxy, which are then used in computing the \cf.
\textbf{The standard technique is unable to account for higher order corrections or interdependence between systematic parameters; these complexities may propagate through analyses and bias galaxy clustering results.}
Other correction methods, including mode projection and Monte Carlo approaches, address nonlinearities to some extent, but lack flexibility and are unable to handle interacting systematics.

\subsection{Motivating new correction methods: adversarial mock catalogs}

Inspired by adversarial machine learning methods, we will construct artificial datasets with deliberately adversarial systematics, alongside the means to defeat them.
These datasets will form a comprehensive testing suite of mock galaxy catalogs.
With these, we will demonstrate the regimes in which current systematics correction methods become insufficient.
These are ``adversarial'' in that there is a tension between hiding systematics in the dataset and detecting these effects, and we can exploit this to improve our detection.
The systematics will be designed to be difficult to detect under current linear methods, and our detection method will be challenged to correct for these systematics. 
We expect to show that our approach to systematics correction addresses some of the insufficiencies of current methods, resulting in less biased measurements and the ability to detect subtle signals in the distribution.

Our testing suite will include toy models as well as catalogs modeled after target surveys, with the appropriate window function.
We will inject systematic biases in density by introducing density variations in the galaxies based on adversarially complex functions of the metadata.
The result will be comprehensive testing suites that cover many systematic effects and range from simple combinations to extreme variation.
\textbf{These adversarial extremes will create discovery potential by showing that systematics may be obscuring physical information.}
This connects strongly to our search for anomalies in the third theme; we require robust systematics calibration to uncover anomalous effects.

\subsection{Defeating adversarial systematics with non-parametrics}

Ideally, systematics and clustering are modeled simultaneously. 
However, we don't have a a tractable likelihood for clustering analyses.
While likelihood-free inference could provide an eventual path forward, for the near-future we can develop more sophisticated methods to calibrate out systematics before performing clustering analyses.
Machine learning methods have already shown promise in systematics removal in other subfields.
Adversarial neural networks have been used to remove the jet background in particle collision data at the Large Hadron Collider \citep{Shimmin2017}.
Gaussian processes (\GP s) have been used to correct for instrumental systematics in exoplanet transit surveys \citep{Gibson2012, Aigrain2016}. 
\GP s lend themselves to this type of problem, as they allow for nonlinear interpolation over the data space; we propose to use GPs to remove systematics in galaxy surveys.

\GP s provide a Bayesian approach to modeling data; they define a prior distribution over a function space, and then use the known data to produce a posterior probability distribution over unknown values. 
In practice, \GP s learn the labels of a training set and generate hyperparameters characterizing the data.
These can then be used to label previously unseen data by performing a nonlinear regression over the training data space.
In our application, we will model the galaxy density and redshift distribution as complex functions of observational metadata, as described above.
\textbf{We will train the \GP\ on the adversarial mock catalog, which will learn the nonlinear variations from the data.
We will then use the \GP\ to predict the functions that generated the systematic effects by performing a regression over the training data space.}
It is then straightforward to correct for these systematics in new surveys by assigning the galaxies weights determined by these functions.

\GP s replace a parametric choice (polynomials or sines and cosines, for example) with a kernel choice.
This kernel choice represents model complexity.
This has to be chosen carefully, since there are kernels that will overfit any systematics, and others that will underfit.
We propose to use leave-out cross-validation to set the kernel parameters.
This is a well-tested, safe method for kernel choice, and is a reliable machinery in many of the \PI's projects.

We expect to show that our \GP\ approach does better at recovering unbiased clustering statistics for a typical level of systematics, compared to traditional methods.
Further, we will perform this test on catalogs with extreme systematics that could be obscuring interesting clustering features; we expect traditional methods to be unable to detect these features, while our method should reveal this information.
Our systematics calibration method will improve current measurements by reducing systematic biases and open up more discovery space in galaxy clustering analyses.

\section{Third theme: Searches for anomalies}

We propose to create and implement a method for reliably finding (or ruling out) anomalies,
or subtle departures of the observed \LSS\ from the \LCDM\ theoretical expectations.

\subsection{Why search for issues in the large-scale structure?}

Contemporary observational large-scale structure science---and
cosmology in general---is very focused on parameter estimation.
The cosmological parameters are known now to incredible precision.
New effects will become visible in the near future, which will add
new parameters to the list and permit us to estimate current
ones even more precisely.
The best-fit (or measured or estimated) parameter values are those near
the peak in the log-likelihood function, or the
(log) probability of the data given the parameters and the fundamental
physical assumptions.
The best possible precision with which the parameters can be measured
is related to the second derivative of that log-likelihood function.
The activity of parameter estimation has the beauty that it is a fully
worked out and justified procedure in classical statistics.

Importantly,
parameter estimation can proceed \emph{whether or not the fundamental theory
is a good fit to the data}.
It depends only that the data be precise, and that the parameters show
effects in our expections for the data.
\emph{But we must also think about the goodness-of-fit,
or the respects in which the data do and do not fit the model}.
This is a problem bigger than parameter estimation, as it involves comparing qualitatively different models.

We are going to simplify these questions of goodness-of-fit to questions about
\emph{anomalies}, or respects in which the data \emph{do not conform}
to the predictions of \LCDM.
We are suggesting this focus on anomalies for two connected reasons.
The first is that there have been proposed or claimed anomalies in the
cosmic microwave background (\CMB) \citep{Huterer2010}, and the conversation around them has been
productive but flawed.
Indeed none of the proposed anomalies---for example the power
asymmetry, the axis of evil, and the cold
spots---is a confident statistical result.
This is partly because these results are subtle.
And partly because they were discovered in an \foreign{ad hoc} way, which is hard to
assess in any \foreign{a posteriori} analysis.
The productivity of these results is to be admired, but the statistical
context needs to be reconsidered.
That's what we propose.

The second reason for a focus on anomalies is the fact that the \LCDM\ model must sure surely be an approximation to a more intricate theory.
Complexity isn't \emph{required}, but it would be odd if the light sector of quarks and leptons and
force carriers would show so much phenomenology and the dark sector of
dark matter would show \emph{none}.
What we know from years of \LCDM\ is that if anomalies exist, they are
hiding: There are no obviously wrong predictions of \LCDM\ known at
scales larger than 10-ish Mpc, and while there are some issues at smaller scales \citep{Bullock2017}, these are
all consistent at the present day with unmodeled baryon dynamics, such as star formation and feedback.
This leads to the conclusion we need to look for subtle effects, or effects that
don't project strongly onto standard two-point-function measurements.

\subsection{Search for weakly broken homogeneity; hypothesis registration}

In some ways the most interesting of the \CMB\ anomalies is the
North-South power asymmetry. It's interesting because it is also
\emph{natural}: If statistical isotropy or homogeneity is weakly
broken, it will show up first as an $\ell=1$ distortion (one half of
the sky different from the other, slightly).
For example, it might be expected in Universes with certain kinds
of primordial non-Gaussianities \citep{Dalal2008} or if our patch is
near (in a Hubble sense) to a domain wall.
This motivates a search for anomalies in the \LSS\ that are small
moves away from statistical homogeneity.
To first order, in any finite survey volume, these will appear as
\emph{gradients} of statistical properties with respect to three-space
position.
These kinds of anomalies are the first for which we will search, not
because we think these anomalies are likely to appear, but because they
are natural, and they are easily enumerated.

This kind of weakly broken homogeneity has been investigated previously,
using various methods.
\cite{Mukherjee2018} looked at the variation of the cosmological parameters in spatial patches.
\cite{Zhai2017} made forecasts for detecting a hemispherical power asymmetry (similar to that seen in the CMB) in redshift surveys.

Even in this small space of anomalies (gradients), there are subtleties. The first
is that there are many stastics to search through.
One way to enumerate these is to list all one-point statistics, then two-point statistics, three-point and so on. As examples, we could look for a gradient in the variance at
$20\,h^{-1}\,\Mpc$ scales; a gradient in the location
of the BAO feature; or a gradient in the inferred value of the amplitude
$\sigma_8$ of the power spectrum.
We need to determine an enumeration of the possibilities (and, preferably, an
ordering; see below).

The second subtlety is that there will be some gradients that are easier
to measure than others. There are trivial considerations, like which
quantities show high signal-to-noise in the data set. But there are also less
trivial considerations, like which kinds of gradients could be imitated by or
masked by systematic effects.
This links this theme to the second theme of modeling systematics:
The framework we build for modeling systematics will tell us which
kinds of anomalies we can securely detect, and at what amplitudes we
can detect them.

So even in this tiny sandbox---looking only at weakly broken
statistical homogeneity---there is significant work to be done.
Our approach is as follows:
\begin{itemize}
\item
Write a comprehensive list of all statistics for which we are going to search
for gradients. This must be chosen in advance.
\item
Choose a survey target. This will be
\SDSSIV\ \eBOSS. Obtain a full set of mocks (which are public, and besides which, we are collaborators on the project), along
with realistic systematic issues and modeled empirically with
systematics models as in the second theme.
\item
Search for gradients in all of the statistics \emph{in the mock
data} using our estimator from the first theme.
These searches should turn up null (since the simulations are
homogeneous). By ``null,'' we mean
that we will get amplitudes and discoveries that are consistent with the noise, or with
information-theoretic expectations.
This process gives an
opportunity to test our code and check expectations.
\item
Perform injection-recovery tests, in which we inject statistical inhomogeneities (our anomalies)
and attempt to recover them.
We will do the injection by deleting and adding galaxies in a
spatially biased way.
The recovery will involve re-fitting of systematics with the methods of the second
theme, because some anomalies will look like systematic issues.
We should find, in the end, that some gradients are better constrained than others.
\item
Pre-register the enumerated list of statistics and the data-analysis code that
searches for them.
This pre-registration might involve a GitHub+Zenodo tag or it could be
a full arXiv publication. It is possible that a complete refereed paper
is merited, given the sophistication that our injection--recovery tests will require.
\item
Operate on the real \eBOSS\ data. Visualize, write up and publish results of the
analysis with the pre-registered code.
\item
If any anomalies appear in that analysis, follow them up with further
investigation, but not contaminating or re-considering any of the results
of the pre-registered analyses.
\end{itemize}

\noindent
We are well-poised to perform this analysis, as \textbf{the ideal tool for
  finding cosmological gradients in power is the continuous
  correlation-function estimator we are building} as described in the first theme.
The estimator is flexible; the basis can be chosen to include not only separation, 
but also a three-space location basis in the Hubble
Volume. That is, every pair of galaxies will have not just a
separation $s$ in (say) redshift space, it will also have a position
(maybe the center-of-mass position $x$) in three-space inside the
survey.
The basis functions in $s$ can be
multiplied by independent linear functions of
$x$ to make a new smooth basis $f(s,x)$. If the Universe is truly
homogeneous, statistically, the part of the basis that depends on $x$
will only be populated at a level expected by noise; all of the
significantly non-zero amplitudes should be on the part of the basis
that is constant in $x$.

\textbf{One challenge of this theme is the pre-registration requirement.}
The arXiv explicitly rejects papers that it regards as ``proposals.'' Will a
pre-registration document or paper be regarded as a proposal? In principle
tagged GitHub code represents a pre-registration. But of course it is possible
to modify past history, so it isn't as secure as an arXiv posting.
We don't expect anyone to attempt to do anything nefarious, but if we are
going to try to be statistically principled, we probably ought to go fully principled!
This is why we propose using Zenodo, since it can snapshot projects in a tagged
state.
Solving the pre-registration problem is explicitly part of what we are proposing to do.
We will work through the possibilities
and find a solution to pre-registering hypotheses that could be used by the whole natural-science
community.
\KSF{I understand why this paragraph is written so speculatively but that does make it weak. I think some ordering and language changes would strenghthen it while still emphasizing that part of the proposal is figuring out how to do this.}

\subsection{More comprehensive, or more theory-motivated searches}

There are many directions to go with anomaly detection.
Our primary proposal goal is to execute a well-defined, statistically sound search for anomalies, using the weakly-broken symmetries (gradients) discussed above.
There are many possible follow-on projects that we could explore if time and enthusiasm of the personnel permit, and depending on the evolving scientific context in the world and at NYU.

One avenue would be to go to larger spaces of anomalies.
In this case, it would be good if the anomalies for which we search can be
lexicographically ordered.
That's because if we want to do a much bigger search, in a much bigger space
than just spatial gradients of two-point statistics, the number of hypothesis
tests will get large.
We expect---if no anomalies are real---that the p-values of the pre-registered searches should
grow in a particular way as the number of tested hypotheses increases.

Another avenue would be to search for highly motivated anomalies, motivated by dark-matter or dark-sector phenomenology work in the theory community.
For example, in theories in which the initial conditions form a glass, there might be phase correlations that don't appear in the power spectrum, but which appear in the correlations among $k$ modes.
For another, there could be dark-sector acoustic modes that appear at scales other than the baryon acoustic scale.
For another, a dark-matter decay product or phase transition in the early universe could leave an imprint at a localized set of scales.
For yet another, there could be tiny asymmetries in left-handed or right-handed composite modes in density--velocity space.
The choice of searches will depend on the direction of theoretical work, and on which theories have observable consequences.
Our framework will be flexible enough to perform these searches in a robust way as predictions come available.

This proposal is to develop the statistical framework for anomaly detection.
This involves enumeration of hypotheses,
null tests and injection tests with mock data and systematics modeling,
and pre-registration of hypotheses and code prior to work on real data.
Once these things are in place, the Hubble Volume is our oyster.

\section{Workshops and pedagogical publications}

In executing the project, the investigators will build curricular
materials aimed at undergraduates transitioning to graduate school.
These will be similar the the pedagogical materials maintained by
the PI on the arXiv, on cosmology, on fitting models, on probability
calculus, and on Markov Chain Monte Carlo (see the PI biographical sketch).
Here we propose to create these in domains that
overlap this project, in particular point processes and estimators.
We have already, in preparing this proposal, learned things about
point processes that are not widely known in physics, such as
how estimators can be made insensitive to window functions.
Our goal is to create materials that help prepare students
in the physical sciences to have better data-science, applied-mathematics, and statistics
skills and therefore better PhD preparations.
Contexts for using these materials include undergraduate-to-PhD bridge programs, courses for advanced undergraduate and early graduate students, and existing research experience programs. 
We will test and refine these curricular materials in multi-day workshops for
students of broad backgrounds in New York City.

Many of the skills critical for a career in astrophysics are not explicitly taught in the college physics curriculum. 
These include data analysis, statistics, and data science, which are fundamental to much of modern astronomy.
Graduate students are expected to pick up these concepts, yet there is a lack of pedagogical materials for understanding them at a high level with applications to astrophysics.
To fill this gap, we will create materials that contain exercises and examples following best pedagogical practices, while also serving as reference materials for students and active researchers.

We will use the pedagogical materials to teach intensive workshops on these topics for our target audience of advanced undergraduates; the PI has experience designing effective workshops \citep{Huppenkothen2018}.
The investigators are well-versed in inquiry-based teaching practices, which are shown to increase understanding, with a focus on equity and inclusivity \citep{Ball2010}.
We will design the workshops using these practices, to ensure that students are building their knowledge regardless of their prior experience.
The investigators have connections with local programs to hold these workshops for, through the Institute for Science and Engineer Educators. 
With feedback from these workshops, we will iterate on the materials, and release them all online as open-source products.

\section{Implementation and management plan}

This project is phrased in terms of three themes, the third of which depends on the
first two.
However, these dependencies are not strict: There is a lot that can be done in each theme
independently of the others.
For this reason, we expect to work along all three themes in all years of the proposal.
However, we will front-load the first theme into the first year, producing a functional
unbinned estimator, and working open-source code, and a paper for the refereed literature in Year One.
We will concentrate most of our effort in Year Two on the systematics modeling, and produce
adversarial mocks, and a \GP-based model, along with open-source code and a paper.
In Year Three we will concentrate on the third theme, bringing together our work from Years One and Two.

In each summer we will organize one 3-day workshop for senior undergraduate students and beginning
graduate students. These will be hosted at the Flatiron Institute at no cost to the \PI, as detailed
in the Facilities and Resources part of this proposal, and as supported in the letter of support from the director of the Center for Computational Astrophysics there.
These workshops will be advertised through our New York networks, and especially the CUNY system, where the Flatiron Institute has a partnership for undergraduates performing astrophysics research.
We will produce three pedagogical contributions, one for each of these summer workshops, which we will test and refine at the workshops before publishing on the arXiv.
These workshops will be sheduled and planned by the \PI, and executed by the full team, including both the \GRA\ and the undergraduate researcher.

In terms of supervision, the cartoon is that the \PI\ will supervise the graduate student and the graduate student will supervise the undergraduate researchers.
Of course the \PI\ will also attend to the hiring and supervision of the undergraduates.
The undergraduate researchers will do appropriate sub-parts of each of the project themes, like for example executing the estimator on mocks and data sets, gathering or creating mock data, or reading theoretical particle physics papers to find implications for anomalies and observables.
Writing of the papers of the project will be the primary responsibility of the \GRA, for whom these papers will form the basis of a dissertation; the \PI\ will supervise this writing.
The distribution and maintainance of the open-source code will be the joint responsibility of the \PI\ and the \GRA.

\section{Prior NSF support}

\paragraph{\acronym{AST-1517237}
\project{New Probabilistic Methods for Observational Cosmology}
(2015--2019) \$328K:} This project supported PhD dissertation work by Alex Malz.

\paragraph{Intellectual merit:}
In this project we developed probabilistic methods for dealing
with noisy and probabilistic outputs from large-scale structure surveys,
especially surveys that rely heavily on photometric redshifts and other
kinds of noisy and contaminated data.
The methods produced by the project make future projects like \LSST\ more
precise and more capable.

Because the methods are general, we also, as part of this grant,
applied related techniques to calibrate \Kepler\ light curves,
and detect planets in the \Ktwo\ Mission data.
Some of the methods we developed have been used subsequently in analyses of the
\NASA\ \Kepler\ and \TESS\ Mission data.
We also applied related and derived methods to stellar spectroscopy,
and to the stellar populations found in the \ESA\ \Gaia\ data.

\paragraph{Broader impacts:}
The grant proposal was co-written by Dr Malz and established his career.
During the award period, Malz brought many statistical innovations into the
cosmology \LSS\ community, and had a big impact on the \NSF-funded \LSST\ Collaboration.
The proposal also supported the creation and publication of several pedagogical contributions
on the arXiv which are used for training advanced undergraduates and graduate students.
The grant supported travel by Malz and Hogg to American Astronomical Society meetings
to organize, participate in, and run \project{Hack Together Day} and travel to
\project{AstroHackWeek} to organize, participate in, and run those meetings too.
Malz helped organize hack days inside the \LSST\ Collaboration as well.

\paragraph{Publications and products:}
The publications supported the following publications:
\citet{2016arXiv160303040C, 2016arXiv161005873V, 2016ApJ...833..262H,
  2017ApJ...837...20P, 2017ApJ...838....5L, 2017AJ....153..257O,
  2017MNRAS.469.2791H, 2017arXiv171002428W, 2017MNRAS.471..722H,
  2017AJ....154..222L, 2017MNRAS.472.4144V, 2018ApJ...853..198N,
  2018ApJ...857..114W, 2018arXiv180407766H, 2018ApJS..236...11H,
  2018AJ....156...18P, 2018AJ....156...35M, 2018AJ....156..145A,
  2018ApJ...867..101B, 2019MNRAS.482.3696C, 2019ApJ...872..115V,
  2019ApJ...881...80L, 2019arXiv190811523D, 2019AJ....158..171M}.
It also supported open-source code development published on \project{GitHub}.

\paragraph{Other \NSF\ support:}
In addition to the highlighted grant, the \PI's group has been supported by
\acronym{OAC-1841594}
\project{Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics} (2018-2020),
and
\acronym{IIS-1124794}
\project{A Unified Probabilistic Model of Astronomical Imaging} (2011--2016)
and
\acronym{AST-0908357}
\project{Dynamical models from kinematic data: The Milky Way Disk and Halo} (2009-2011),
and
\acronym{AST-0428465}
\project{Automated Astrometry for Time-Domain and Distributed Astrophysics} (2004--2007),
and
\acronym{PHY-0101738}
\project{Theoretical Particle Physics, Astrophysics and Cosmology} (2001--2004).
and (in ancient history) a \NSF\ graduate fellowship.
The majority of the \PI's 220 refereed publications have been at least partially supported
by the \NSF.

\section{Relevance to \NSF\ priorities and programs}

In addition to being relevant to \NSF\ \acronym{AAG} Program, this
project also \textbf{connects strongly to the \NSF\ \acronym{CDS\&E}
program elements and fits into this program}.
This proposal is literally to ``promote the creation,
development, and application of the next generation of mathematical,
computational and statistical theories and tools that are essential
for addressing the challenges presented [by] \ldots the explosion and production of digital
experimental and observational data.''
We are developing statistical tools that are imperative to address the complexities of the upcoming deluge of large-scale structure survey data.
We also match the other \acronym{CDS\&E} criteria:
We will ``create, develop, and apply novel \ldots statistical methods, algorithms, software,'' and will make these available to the community so they can become part of standard large-scale analyis pipelines.
With our hypothesis registation we also ``encourage adventurous ideas that generate
new paradigms.'' In short, this proposal fits extremely well with the \acronym{CDS\&E} program
criteria.

In addition, this propoal connects to several of the \NSF\ 10 Big Ideas themes:
This proposal is explicitly motivated by changes to the data landscape,
so it it connects to \textbf{Harnessing the Data Revolution}.
For the large-scale structure analyses we are focusing on, as we look to larger volumes, we look
to noisier tracers, through more foregrounds, and superimposed on
stronger backgrounds. This proposal is to address these challenges
directly.
The work we are doing on correlation-function estimators connects
directly to literature on point-processes, and the work on calibration
connects to the machine-learning literature. Thus we are
\textbf{Growing Convergence Research}.
For example, we have found almost no citations to the point-process
literature in the cosmological literature, and almost no citations to
the cosmology literature in the point-process literature.
We're fixing that problem!
While this project is not explicitly multi-messenger, it supports
multi-messenger goals around the dark sector and dark matter, which
connects to \textbf{Windows on the Universe}.
If we find anomalies, they might have implications for direct (laboratory)
detection of dark matter, production of dark matter in colliders, or
annihilation. If dark matter is composed of compact objects, this work
will eventually connect to gravitational-wave astronomy. These
connections are speculative, but they could pay off immensely in the
new multi-messenger era.

\section{Intellectual merit}

The \LCDM\ model is exceedingly successful!
It is an incredible approximation to reality; indeed it earned the 2019 Nobel Prize in Physics.
But we know that it is likely to be wrong in detail:
The dark sector is unlikely to have no structure or interactions at all.
Indeed, since it is the majority of the mass, why wouldn't it contain the majority
of the physics?
Whether or not it turns out to be an approximation, it is extremely important that
the physics community test this model.

This project is designed to test the \LCDM\ model.
Even more importantly, this project is to build tools that empower anyone to test the model!
We will deliver both general methods and specific, released,
open-source code that works efficiently on current data and can be
incorporated into current and future analysis pipelines.
Aligned with the \acronym{CDS\&E} objectives, we will reduce the computational requirements and
increase the capabilities of current and future large cosmological surveys.
That is, this project will make NSF-funded cosmology projects (including
\SDSSIV, \DESI, and \LSST) more productive, more precise, and more
capable of making ground-breaking discoveries.

This proposal will also connect and add value to advances in wide-ranging areas of traditional cosmological measurement.
Recent theoretical work has shown that new effects will become potentially observable with the next increase in precision.
For example, it's looking likely that we can measure the total mass in
neutrinos just from observations of \LSS\ and the
\CMB.
This would answer questions that direct mass measurement experiments 
can't yet answer.
As another example, if the cosmological constant turns out to be a particle rather
than a term in general relativity,
this signature would appear at sub-percent level in the
expansion history and the growth of structure.

Finally, this project creates new statistical methods and capabilities for the 
cosmology community.
We will establish methods for generation, enumeration, testing, and pre-registration of hypotheses that
will be of general value in cosmology and other
sciences.

\section{Broader impacts}

The pedagogical publications and associated workshops we will produce through this proposal are designed to address the attrition of underrepresented minority students in physics.
The largest drop in continuation of a career in astrophysics for underrepresented minorities is at the undergraduate-to-PhD level.
While 10\% of Physics bachelor degree recipients are Hispanic American or African American, only 3\% of Physics PhDs are granted to people belonging to these groups \citep{Ivie2018}.
Our goal with these materials and workshops is to provide supportive learning environments and fundamental skills for students at this critical career junction, to ensure that all students are prepared for the transition to graduate school.

This proposal will provide the opportunity for significant research mentorship.
The \PI\ will supervise the \GRA's research, and the \GRA\ will mentor undergraduate researchers.
Research experiences with quality mentorship have been shown to be critical to persistance in \acronym{STEM} for undergraduates from underrepresented backgrounds \citep{Estrada2018}.

The interdisciplinary nature of this research means that the educational environment for the \GRA\ and the undergraduate researchers is unique and valuable, especially now.
Training in methods, statistics, applied mathematics, and data science are all critical to work in almost any computational scientific or engineering context.
The former members of the \PI's group work in a wide array of industries, including academic, finance, online commerce, journalism, and public policy.
Training of a diverse group of early-career scientists for a new world of technical work is one of the broader impacts of the work proposed in this proposal.

Finally, the research products from this work will have an impact beyond astrophysics.
The correlation function is commonly used across other disciplines, from examining patterns in DNA sequences to time-correlated expenditure in econometrics.
The statistical methods we develop for \cf\ analyses will be quite general could be formulated for these applications, allowing for improved understanding in a multitude of fields.

\section{Not-so-frequently asked questions}

\paragraph{Why does this proposal have two authors?}
Hogg is the \PI; Kate Storey-Fisher is the graduate student at NYU who would be supported on this grant as the \GRA.
Storey-Fisher contributed substantially to the writing of this proposal; she is a co-author, a proposed beneficiary, and a supervisor (for the undergraduate researcher) of this project.

\paragraph{Isn't it ridiculous to look for anomalies in the large-scale structure?}
No! Okay well to some extent yes. But it is precisely when a theory or model is most successful that it's predictions become most critical to test. Besides, we \emph{do} think that \LCDM\ is an \emph{approximation} to the true theory.

\paragraph{How could a new estimator help with the cost of simulations?}
One of the most limiting aspects of \cf analyses is the number of mock catalogs required for error estimation, which are built from cosmological simulations. This depends on the number of bins or basis functions; our estimator reduces this without sacrificing accuracy, significantly reducing cost.

\paragraph{Don't you still have to make choices about your basis functions for your estimator, so you're not totally eliminating binning?}
Yes! There are still choices. But those choices don't require hard-edged bins, and don't require that we approximate smooth functions with clunky boxes.

\paragraph{What is the connection between systematics correction and anomaly detection?}
These are highly related, because an anomaly might be ``explainable'' as a systematics issue, or an uncorrected systematic might appear as an anomaly. So we have to do both well, simultaneously, if we want to be conservative.

\paragraph{Will pedagogical publications and workshops really have a broad impact?}
The materials we produce are designed to impact students at a critical point in their careers.
Access to these materials, alongside well-taught workshops, have the potential to give students with less experience and resources the necessary grounding to do high-quality research in data-driven astrophysics.
Further, some of the \PI's pedagogical contributions (especially on cosmological distances and fitting models) are highly cited.

\clearpage
\bibliography{description, prior}
\bibliographystyle{apj}

\end{document}
