% This file is part of the GetRichOrDieTrying / KSF project
% Copyright 2019 the authors.

% to-do
% -----
% - draft all sections

\documentclass[12pt, fullpage, letterpaper]{article}
\usepackage{fancyheadings}
\input{hogg_nsf}

% headers and footers
\setlength{\headsep}{2ex}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\newcommand{\thistitle}{\acronym{CDS\&E}: Betterizing the next generation of cosmology surveys}
\lhead{\textcolor{darkgrey}{\textsf{Hogg \& Storey-Fisher / \thistitle}}}
\rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
\cfoot{}

\begin{document}

The cold-dark-matter-with-a-cosmological-constant (\LCDM) model is
very well established observationally, having explained many different
kinds of data with a small number of free parameters, and having made
some ab-initio predictions (like the baryon acoustic feature; CITE)
prior to the observations.
It's almost certainly \emph{not the true model}.
It seems unlikely that the dark sector is a single species, with no
non-gravitational interactions, and no interesting thermodymamic or
dynamical features.
However, what is well established is the following:
Whatever \emph{is} the true model \emph{is very well approximated} by
\LCDM\ over a wide range of scales in length and density.
So the \LCDM\ deserves our respect and our attention.
And it also deserves our criticism and our scrutiny.
Here we proceed with both.

In addition to the importance and success of the \LCDM\ model---or
really because of it---the principal cosmological parameters (which
can be thought of as the age, the densities in various components, and
the power spectrum of the initial conditions) are measured in data
with percent-level precision.
And maybe the over-arching theme of astrophysics, historically, is
that with new precision comes new opportunities!
The current and next generations of surveys hope to
make measurements of effects which enter at sub-percent levels:
For example, it's looking likely that we can measure the total mass in
neutrinos just from observations of large-scale structure and the
cosmic microwave background, when this is truly a trace component,
dynamically.
This would answer questions that direct mass measurement experiments
can't answer yet, or ever if there are contributions from sterile
neutrinos.
For another example, if the cosmological constant turns out to be a particle rather
than a term in general relativity, we now know, observationally, that
the differences that makes will appear at sub-percent level in the
expansion history and the growth of structure.

For us, these are just examples of things we might be doing in the
coming decade.
Here we propose to design, build, and exercise tools that will be
valuable no matter what the questions of cosmology become.
In our view, given the success and precision of the current
cosmological model, new projects either have to obtain
\textbf{substantially more precision}, or else look for subtle
\textbf{departures from predictions}.
That is, we want to see small effects, and in particular, small
effects that might call into question the deep assumptions of the
\LCDM\ model.

Doing better than has ever been done before will require work along
multiple axes.
The first axis is \emph{volume}: We need to observe more volume. This
is happening now, with projects like HOGG, KSF: PROJECT LIST WITH
CITATIONS.
But of course this new volume comes at a cost: As we look to larger
distances and larger volumes with denser tracers, we push our
observations to noisier targets with less-well-measured individual
properties.
That is, as the data get better in a global sense (more volume, more
modes measured), it gets worse in an individual-object sense (fewer
bits per target, more contamination from foregrounds and backgrounds).

This all leads to the second axis along which we have to work, which
is calibration, contamination, and window functions:
As the data get harder to use at the individual-object level, it is
harder to know that we have surveyed the volume uniformly, or, more
importantly, do we understand the non-uniformity of our sampling?
Survey window function knowledge starts to depend on things about the
observational projects that we can't know well, like small-scale
variations of the point-spread function or temporal variability of the
photometric bandpasses.
In this context we are pushed to data-driven techniques for
understanding the selection, which connect to ideas of
self-calibration, where the PI has been a pioneer.

And the third axis is statistical sophistication.
As we simultaneously move to higher precision requirements, and larger
volumes of data, and with lower-precision individual-object data, it is 
easier and easier to get fooled.
This fooling can come in the form of systematic effects (like
photometric calibration or foregrounds) in the data projecting onto
tiny physical effects (like neutrino masses) in the clustering.
Or it can come in the form of anomalies appearing: If the volume is
huge, the effective size of any search is huge.
We \emph{will} discover anomalies; the hard question will be whether
they are statistically significant or worth further study.
This gets into issues of multiple hypothesis testing, which we will
discuss below.
But these two issues directly combine, because unmodeled or
incorrectly modeled systematic issues in the data will project onto
discoverable anomalies.
So if we are going to search the next generation of surveys for
interesting new departures from baseline theoretical expectations,
we better have a very solid statistical plan.

This proposal is to build and exercise general-purpose tools that will
make the next generation of (galaxy, quasar, and intensity-mapping)
large-scale structure (\LSS) surveys more precise, more powerful, and
more capable of making truly new discoveries.
The project has three themes, which connect to the three axes above in
various ways.
In the first theme, we replace the standard estimator for clustering or
correlation functions with something far more flexible, which permits us
to do more with more cosmological volume at fixed computation.
In the second, we analyze, attack, and re-build the self-calibration 
methods for window function estimation.
In the third, we perform a very false-positive-safe search for anomalies
in the large-scale structure, to identify departures that might point to
new physics.

\section{First theme: A new estimator for galaxy clustering}

KSF: I have copied in what I wrote in the summary, but can ou replace
this with two-ish pages about what we are doing and why?

The project will build and operate a new estimator for
galaxy clustering that obviates binning of objects (galaxies or
quasars or pixels) or pairs into bins; instead it estimates continuous
functional forms for clustering as a function of scale, galaxy mass,
color, and so on.
These continuous functional forms make a better representation of the
clustering than any binning; therefore this estimator can do more with
fewer clustering-model components.
This in turn lets it make more powerful measurements with less data,
and with less of the computation that is used to do uncertainty
propagation.

Call out \textbf{Intellectual merit}.
This project will deliver both general methods and specific, released,
open-source code that works efficiently on current data and can be
incorporated into current and future analysis pipelines. And: less computing!

Call out \textbf{Broader impacts}.

\section{Second theme: Adversarial approaches to systematics}



The project will build adversarial simulated data that is
designed to defeat current methodologies for finding and correcting
systematic effects of calibration and target selection in \LSS\ 
surveys. And then build far more general calibration
programs, based on non-parametric models, that can defeat the
adversaries.

Call out \textbf{Intellectual merit}.
This project will make NSF-funded cosmology projects (including
\SDSSIV, \DESI, and \LSST) more productive, more precise, and more
capable of making ground-breaking discoveries.

\section{Third theme: Searches for anomalies}

Contemporary observational large-scale structure science---and
cosmology in general---is very focused on parameter estimation.
And rightly so!
The cosmological parameters are known now to incredible precision.
And new effects are coming visible in the near future, which will add
new parameters to the list and permit us to estimate current
parameters even more precisely.

In addition, parameter estimation is a very well-defined activity:
Parameter estimation proceeds by performing likelihood-based or
Bayesian inferences.
The best-fit (or measured or estimated) parameter values are those at
(or near) the peak in the log-likelihood function, or the (log)
probability of the data given the parameters and the fundamental
physical assumptions.
The best possible precision with which the parameters can be measured
is related to the Fisher information, which is (in turn) the second
derivative of that log-likelihood function.
The activity of parameter estimation has the beauty that it is a fully
worked out and justified procedure in classical statistics, and it
delivers results that can be judged in the framework of information
theory.
And we are really, really good at it.

Interestingly and importantly to our story,
parameter estimation can proceed \emph{whether or not the fundamental theory
is in fact a good fit to the data}.
That is, it doesn't depend on the model being \emph{good}.
It depends only on the data being precise, and the parameters showing
effects in our expections for the data (as encoded by the log-likelihood function).

But we believe that it makes sense to think about the goodness-of-fit,
or the respects in which the data do and do not fit the model.
This question is as old (in statistics) as the question of parameter
estimation, and older, even in the history of cosmology.
But it is a harder question, because it involves comparing
qualitatively different models or paradigms.

We are going to phrase these questions of goodness-of-fit in terms of
\emph{anomalies}, or respects in which the data \emph{do not conform}
to the predictions of the dominant paradigm.
We are suggesting this focus on anomalies for two connected reasons.
The first is....productivity of CMB anomalies

The second reason for the focus is....the \LCDM\ model is surely an
approximation!

HOGG: MAKE SURE ALL THIS GETS SAID IN THIS SECTION, OR MODIFY THE SUMMARY:
The project will build methodologies that perform (and perform
ourselves) statistically principled brute-force searches of \LSS\ 
surveys (current and future) for theory-motivated departures from
predictions of the \LCDM\ model.
The cosmological model is very successful.
But the discovery of a departure at intermediate-to-large scales
(where the physical model is extremely accurate) would be so important
and significant that it is worth investing effort here.
The principled aspects of the project will include pre-registration of
hypotheses (that is, an enumeration of the specific forms and kinds of
deviations) prior to search.
And this tool category is related to the previous two, because we need
to distinguish small departures from systematic effects, and we need
to extract signatures at bound-saturating precision.

HOGG: Call out \textbf{Intellectual merit}.
This project will establish methods for pre-registration of hypotheses that
will be of general value in cosmology and other areas in the natural
sciences.

HOGG: Call out \textbf{Broader impacts}.

\section{Workshops and pedagogical publications}

In executing the project, the investigators will build curricular
materials for undergraduate-to-PhD bridge programs, preparing students
in the physical sciences to have better data-science and statistics
skills and therefore better PhD preparations.
We will test and refine these curricular materials in workshops for
students of broad backgrounds in New York City.

Call out \textbf{Intellectual merit}.

Call out \textbf{Broader impacts}.

\section{Prior NSF support}

Here we list recent \NSF\ grants that have supported the PI and say a
few words about each of them.
In addition to these projects supporting PI Hogg, it is also the case
that Storey-Fisher WAS SUPPORTED PARTIALLY BY WHAT PREVIOUSLY?

\paragraph{\acronym{OAC-1841594}: Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics (2018-2020):}
This one-year (plus extension) grant (for \$36K) is part of...

\paragraph{\acronym{AST-1517237}: New Probabilistic Methods for Observational Cosmology (2015--2019):}
This grant (for \$328K) supported...

\paragraph{\acronym{IIS-1124794}: A Unified Probabilistic Model of Astronomical Imaging (2011--2016):}
This grant (for \$675K) was a Cyber-Enabled Discovery Type I Grant which supported...

\paragraph{\acronym{AST-0908357}: Dynamical models from kinematic data:\ The Milky Way Disk and Halo (2009-2011):}
This grant (for \$147K) supported...

\section{Relevance to NSF priorities and programs}

In addition to being relevant to \NSF\ \acronym{AAG} Program, this project also
connects strongly to the \NSF\ \acronym{CDS\&E} program.
HOGG: What more should be said here?

This project touches on the following three \NSF\ themes:

\paragraph{Harnessing the Data Revolution:}

\paragraph{Growing Convergence Research:}

\paragraph{Windows on the Universe:}

\section{Not-so-frequently asked questions}

We recognize that reviewing NSF proposals can be challenging.
For this reason, we have tried to anticipate and answer some of the
questions that you, as a reviewer, may have, after reading this
proposal.

\textit{How is this a \acronym{CDS\&E} proposal?}

\textit{Isn't it ridiculous to look for anomalies in the large-scale structure?}

\textit{Your clustering estimator seems so general, why are you spending so many words talking about how it relates to simulations?}

\textit{KSF: etc..?}

\end{document}
