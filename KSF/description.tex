% This file is part of the GetRichOrDieTrying / KSF project
% Copyright 2019 the authors.

% to-do
% -----
% - draft all sections

\documentclass[12pt, fullpage, letterpaper]{article}
\usepackage{fancyheadings}
\usepackage{xspace}
\input{hogg_nsf}

% headers and footers
\setlength{\headsep}{2ex}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\newcommand{\shorttitle}{CDS\&E \ldots precision and discovery in cosmology experiments}
\lhead{\textcolor{darkgrey}{\textsf{Hogg \& Storey-Fisher / \shorttitle}}}
\rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
\cfoot{}
\newcommand{\KSF}[1]{\textcolor{teal}{KSF says: #1}}
\newcommand{\HOGG}[1]{\textcolor{orange}{HOGG says: #1}}
\newcommand{\cf}{2pcf\xspace}
\newcommand{\LS}{LS\xspace}
\newcommand{\inv}{^{-1}}
\newcommand{\T}{^{\mathsf{T}}}

\begin{document}

The cold-dark-matter-with-a-cosmological-constant (\LCDM) model is
very well established observationally.
It has explained many different
kinds of data with a small number of free parameters, and has made
some significant ab-initio predictions (like the baryon acoustic feature; CITE)
prior to the observations.
However, it's almost certainly \emph{not the true model}.
It seems unlikely that the dark sector is a single species, with no
non-gravitational interactions, and no interesting thermodymamic or
dynamical features.
However, what is well established is the following:
Whatever \emph{is} the true model \emph{is very well approximated} by
\LCDM\ over a wide range of scales in length and density.
The \LCDM\ model, then, deserves to be worked out, tested, and scrutinized at the
limits of what's possible (or at least sensible!)

In addition to the importance and success of the \LCDM\ model---or
really because of it---the principal cosmological parameters have been
measured in data with percent-level precision.
These parameters can be thought of as the age, the densities in
various components, and the power spectrum of the initial conditions.
However, our percent-precision measurements are not good enough to
detect potential deviations from \LCDM\, or to understand subtle
cosmological and astrophysical effects.
The history of astrophysics has demonstrated, time and again,
that with new precision comes new opportunities.
The current and next generations of surveys hope to
make measurements of effects which enter at sub-percent levels,
which may lead to new physical insights.
For example, it's looking likely that we can measure the total mass in
neutrinos just from observations of large-scale structure and the
cosmic microwave background, when this is truly a trace component,
dynamically.
This would answer questions that direct mass measurement experiments
can't answer yet (or will ever be able to, if there are contributions from sterile neutrinos).
As another example, if the cosmological constant turns out to be a particle rather
than a term in general relativity, we now know that
this signature cannot appear at percent level or greater (in the
expansion history and the growth of structure).

\textbf{Here we propose to design, build, and exercise tools for the
analysis of large-scale structure that will
  be valuable for answering very precise cosmological questions}, even
as the questions themselves evolve and expand.
Given the success and precision of the current
cosmological model, new projects either have to obtain
\textbf{substantially more precision}, or else look for subtle
\textbf{departures from predictions}.
That is, we want to see small effects, and in particular, small
effects that might call into question the deep assumptions of the
\LCDM\ model.

Measuring things better than they have ever been measured before will require work along
multiple axes.
The first axis is \emph{volume}: We need to observe more spatial volume, and with new kinds
of tracers.
This is happening now, with projects like the
\project{e\acronym{BOSS}} survey within the \project{Sloan Digital Sky
  Survey \acronym{IV}} (\SDSSIV) surveying the largest volume to date
\citep{Dawson2015}.
Upcoming wide-field surveys include \DESI\ 
\citep{Aghamousa2016}, which will begin taking data in 2019, followed
by \NASA\ \project{Euclid} \citep{Amiaux2012} and
\LSST\ \citep{Ivezic2018} in 2021, and \NASA\ \project{\acronym{WFIRST}} in the mid-2020s
\citep{Akeson2019}.
These will observe 10 to 20 times more galaxies than current surveys,
but this new volume comes at a cost. As we look to larger
distances and larger volumes with denser tracers, we push our
observations to noisier targets with less-well-measured individual
properties, with more elaborate window functions and hard-to-model foregrounds.
That is, as the data get better in a global sense (more volume, more
modes measured), it gets worse in an individual-object sense (fewer
bits per target, more contamination from foregrounds and backgrounds).

This leads to the second axis along which we have to work, which
is calibration, contamination, and window functions.
As the data get harder to use at the individual-object level, it is
harder to know that we have surveyed the volume uniformly, or, more
realistically, that we understand the non-uniformity of our sampling.
Survey window function knowledge starts to depend on things about the
observational projects that we can't know well, like small-scale
variations of the point-spread function or temporal variability of the
photometric bandpasses.
In this context we are pushed to data-driven techniques for
understanding the selection.
This connect to ideas of
self-calibration, where the PI has been a pioneer.

The third axis is statistical sophistication.
As we simultaneously move to higher precision requirements, larger
volumes of data, and lower-precision individual-object data, it is 
easier and easier to get fooled.
This fooling can come in the form of systematic effects (like
photometric calibration or foregrounds) in the data projecting onto
tiny physical effects (like neutrino masses) in the clustering.
Alternatively it can come in the form of anomalies appearing: If the volume is
huge, the effective size of any search is huge.
We \emph{will} discover anomalies; the hard question will be whether
they are statistically significant or worth further study.
This gets into issues of multiple hypothesis testing, which we will
discuss below.
But these two issues---systematic effects and multiple hypothesis
testing---directly combine, because unmodeled or incorrectly modeled
systematic issues in the data will project onto discoverable
anomalies.
So if we are going to search the next generation of surveys for
interesting new departures from baseline theoretical expectations,
we need to have a very solid statistical plan.

This proposal is to build and exercise general-purpose tools that will
make the next generation of (galaxy, quasar, and intensity-mapping)
large-scale structure (\LSS) surveys more precise, more powerful, and
more capable of making truly new discoveries.
The project has three themes, which connect to the three axes above in
various ways.
In the first theme, we replace the standard estimator for clustering or
correlation functions with something far more flexible, which permits us
to do more with more cosmological volume at fixed computational cost.
In the second, we analyze, attack, and re-build the self-calibration 
methods for handling systematics and window-function estimation.
In the third, we perform a very false-positive-safe search for anomalies
in the large-scale structure, to identify departures that might point to
new physics.
These three themes work together to create new measurement and discovery space,
increasing the scientific reach of the big investments we (as a community)
are making in large-scale structure.

\section{First theme: A new estimator for galaxy clustering}

The project will build and operate a new estimator for
galaxy clustering that obviates binning of objects (galaxies or
quasars or pixels) or pairs into bins; instead it estimates continuous
functional forms for clustering as a function of scale, galaxy mass,
color, and so on.
These continuous functional forms make a better representation of the
clustering than any binning; therefore this estimator can do more with
fewer clustering-model components.
This in turn lets it make more powerful measurements with less data,
and with less of the computation that is used to do uncertainty
propagation.

\subsection{The Two-Point Correlation Function}

Galaxy clustering is one of the most important tracers of the large-scale structure of the universe.
The distribution of galaxies contains information about the universe's evolution and fundamental components, as well as the evolution of the galaxies themselves.
To study the clustering of galaxies, we use observational galaxy surveys that range from hundreds to millions, and soon billions, of galaxies, and are pushing to even higher redshifts.
\textbf{With the increased size and complexity of next-generation galaxy surveys, the tools currently used to characterize galaxy clustering will not be sufficient}.
We propose to address this problem by developing new methods for the analysis of galaxy clustering from observational surveys.

The distribution of galaxies in a volume of sky can be modeled as a spatial point process.
We can use the statistics of point processes to characterize their clustering, giving us insight into the physical processes that produce this distribution.
One of the most common statistics used to analyze galaxy surveys is the two-point correlation function (\cf). 
\textbf{The \cf is critical for inferring certain cosmological parameters, determining the distance scale in the early universe, and constraining galaxy formation by examining how clustering depends on galaxy properties.}
 
The canonical estimator for the galaxy \cf, originally proposed by \cite{LandySzalay1993} (\LS), estimates the excess number of galaxies in bins of spatial separation $r$, compared to a statistically random distribution.
Given a set of $K$ bins, the \LS estimator counts the number of galaxy pairs $DD$ with a separation in each bin, and counts ``random'' pairs $RR$ from a uniformly distributed catalog  to correct for the survey boundary, as well as cross-pairs $DR$.
The estimator is then defined as
\begin{equation}
\xi(r) = \frac{DD(r) - 2DR(r) + RR(r)}{RR(r)},
\end{equation}
where the terms are normalized vectors of pair counts in each bin, each having length $K$.

While the \LS estimator has better overall variance properties than the naive clustering estimator, it still suffers from several limitations. 
Choosing the number of bins requires a trade-off between bias and variance.
Furthermore, for any finite binning in separation, this estimator contains a statistical bias when applied to highly clustered data (e.g. \citealt{Kerscher1998}).
The \LS estimator also can't account for properties besides spatial separation.
Another binned dimension is often used to characterize the dependence
on galaxy properties: the galaxies are divided into subsamples to
analyze the relationship between their properties and clustering.
One tool developed to address this is the marked correlation function, in which galaxies are weighted by the desired property.
Li \& White (2010) showed that luminosity-weighting can place tighter constraints on galaxy formation, but this approach still requires $r$-binning and doesn't account for the survey window.
While these estimators have been sufficient for past analyses, an improved estimator is necessary for future surveys.
\HOGG{I think maybe we should also anticipate and knock down a
  ``kernelized'' correlation function estimator that would smooth the
  field. Also, let's audit for the word ``smooth''. We return a smooth
  function, but we have not SMOOTHED the corr fn at all. Indeed, our
  estimator is technically a deconvolution, although I don't want to
  use that word.}

\subsection{The Continuous-Function Estimator}

\HOGG{KSF: I think we need to give the math / formula for the estimator and say a few words about why the math has the form it does. I can help.}

In order to avoid the issues introduced by finite binning with current estimators, we have developed an approach based on continuous basis functions. 
Inspired by linear least-squares fitting, we replace the scalar pair counts of \LS with a vector of the values output by the basis function, and the normalization term by a tensor of these values.
We generalize the \LS estimator above to any set of $K$ basis functions $f$ of the pair, so we now have
\begin{eqnarray}\displaystyle
DD &\equiv& \sum_{n n'} f(T_n, T_{n'}) \\
DR &\equiv& \sum_{n m} f(T_n, T_{m}) \\
RR &\equiv& \sum_{m m'} f(T_m, T_{m'}) \\
QQ &\equiv& \sum_{m m'} f(T_m, T_{m'}) \cdot f\T(T_m, T_{m'}),
\end{eqnarray}
where $DD$, $DR$, and $RR$ are vectors of length $K$, $T$ refers to the data (position and other information) for the given tracer, and $QQ$ is a $K$-by-$K$ matrix defined as the outer product of basis function evaluations of the random--random pairs.

From these, we can compute the \cf:
\begin{eqnarray}\displaystyle
a &\equiv& QQ\inv \cdot (DD + 2DR - RR) \\
\xi(T_l, T_{l'}) &\equiv& a\T \cdot f(T_l, T_{l'})
\end{eqnarray}
where $a$ is a $K$-vector of the ``amplitudes'' of the basis functions, and $T_l$ and $T_{l'}$ contain the data values at which to evaluate $\xi$.
Thus our estimator allows $\xi$ to be evaluated at \emph{any} separation, as well as at any other value of the data property used in the basis functions.
We have also shown that it is invariant under affine transformations.

This generalized two-point estimator removes the need for binning as the basis functions can be continuous.
For the case of tophat (rectangular) basis functions, this reduces to the standard Landy-Szalay estimator.
A set of basis functions can be chosen specific to the scales, features and properties of interest.
For example, higher order spline basis functions could be chosen to achieve a smooth statistic. 
A set of functions that select expected features in the correlation function, such as the baryon acoustic feature, would allow for a more direct measurement of that feature.
Additionally, basis functions can depend on other properties of the tracer, such as galaxy luminosity or redshift.
Thus our estimator not only removes the need for binning in spatial separation, but can also be used where subsample approaches are currently needed.

We have implemented an initial version of the continuous-function correlation function estimator.
The estimator is built in C within the \texttt{corrfunc} package \citep{Sinha2017}, which performs fast pair-counting on a lattice.
To test the estimator, we have generated lognormal catalogs with a known correlation functions. 
\textbf{We expect these to show that, compared to the Landy-Szalay
  estimator, the continuous-function estimator produces an estimate with lower bias and
  lower variance with respect to the true correlation function.}
Given the enforced smoothness, this estimator should be able to not only produce a more accurate correlation function, but do so with fewer basis functions.
This means that fewer mock catalogs will be needed to produce a covariance matrix, reducing the computational cost of computing the \cf.
The expensive simulations required to build a set of mock catalogs for each dataset is one of the limiting factors in correlation function analyses.
Our estimator will significantly reduce this requirement and allow for faster and more accurate analyses.

Alongside this work, we have revisited the point-process literature (at the intersection of mathematics and statistics) and identified how the biases from binning and edge correction are entering. 
We will use this understanding to develop an estimator that doesn't suffer from this bias.
We will combine these two approaches to develop and implement an estimator for galaxy clustering that is fundamentally unbiased and can be tailored for specific science use cases.

\subsection{Applications: The BAO Peak and Galaxy Evolution}

The clustering of galaxies is a critical probe of the baryon acoustic feature. 
This is a signature of Baryon Acoustic Oscillations (BAO) in the early universe, acoustic waves of photon-baryon fluid that traveled through the universe. 
When the universe cooled enough to cause their decoupling, extra matter was deposited at the scale corresponding to the distance travelled. 
Today this overdensity should be at a scale of around 120 Mpc/h, and indeed we see a peak there in the correlation function. 
The exact BAO scale can be extracted from the binned \cf by comparing it to theoretical models of the correlation function. 
With our estimator, we can directly project the galaxy pairs onto these models. 
Specifically, we will use as one of our basis functions a canonical model of the correlation function with the Planck best-fit cosmological parameters. 
The other basis functions are derivatives of this model with respect to the parameters we are interested in, in this case the matter density, baryon density and spectral index.
\textbf{This eliminates potential error due to the arbitrary choice of bins, and allows for a direct estimation of the cosmological parameters and the BAO scale.}
We will compare this method to the traditional technique, and expect to show that this method produces more precise measurements of the sound horizon with fewer components.

Galaxy evolution is one of the biggest open research topics in modern astrophysics.
The two-point correlation function allows us to ask questions about how the formation of galaxies is related to the evolution of structure in the universe.
The dependence of galaxy clustering on the properties of galaxies, such as their line luminosity and star formation rate, gives insight into this relation.
Our proposed estimator will allow us to extract even more information about this dependence. 
We plan to construct basis functions that contain a dependence on the galaxy property, allowing for a combined encoding of both separation and luminosity. 
We can then quickly evaluate the estimator at any set of luminosities and produce a smooth correlation function for each. 
We will compare this method to the common subsampling approach; \textbf{we expect to show that this method will allow us to interpret the luminosity dependence in more detail and with higher precision.}

Call out \textbf{Intellectual merit}.
This project will deliver both general methods and specific, released,
open-source code that works efficiently on current data and can be
incorporated into current and future analysis pipelines. And: less computing!

Call out \textbf{Broader impacts}.

\section{Second theme: Adversarial approaches to systematics}

The project will build adversarial simulated data that is
designed to defeat current methodologies for finding and correcting
systematic effects of calibration and target selection in \LSS\ 
surveys. And then build far more general calibration
programs, based on non-parametric models, that can defeat the
adversaries.

To remove biases that enter through systematic effects, we are developing an approach inspired by adversarial machine learning methods. 
We will generate survey-specific catalogs with ``adversarial'' data that contains density variations due to nonlinear combinations of systematics.
We will then use probabilistic machine learning techniques to interpolate these nonlinear functions.

\subsection{Background: Systematics correction in galaxy surveys}

To make accurate cosmological measurements and detect potential deviations from the predicted model, we require not only robust statistics, but clean, unbiased datasets on which to apply them.
Observed galaxy catalogs are necessarily imperfect due to systematic effects in observation and instrumentation. 
In order to characterize the clustering from these surveys, we must remove these effects.

Systematics are any issues with the data that produce variations in the density or redshift distribution of the sample, therefore imprinting non-cosmological signals on large-scale structure analyses.
For instance, the galaxy number density varies with the width of the point spread function (PSF), as observations with a wider PSF may obscure galaxies.
Even small systematic effects, such as those from airmass and stellar contamination, can bias large surveys. 
Objects in high-redshift surveys may be particularly sensitive to issues like color selection and instrument calibration.
Current methods assume simple systematics models that don't capture nonlinearities in these effects.
While linear corrections are adequate for most of today's surveys, next-generation surveys will have a combination of more complicated window functions, noisier foregrounds, and complex observational and instrumental effects.
Failure to fully correct for systematics in future analyses will propagate through clustering analyses and give biased results.

One common approach to correcting survey systematics is with regression-based techniques (see e.g. \citealt{Ross2010}). \KSF{At least mention other methods (mode projection and monte carlo)}
Weights for important systematic parameters are determined by performing a fit, typically linear or quadratic, to adjust for dependencies between galaxy density and each parameter.
These are combined into a single weight for each galaxy, which are then used in computing the \cf.
\textbf{The standard technique is unable to account for higher order corrections or interdependence between systematic parameters; these complexities may propagate through analyses and bias galaxy clustering results.}

\subsection{Motivating new correction methods: adversarial mock catalogs}

In order to address complex systematics, we will build a comprehensive testing suite of mock galaxy catalogs, injected with artificial systematic effects.
With these, we will demonstrate the regimes in which current systematics correction methods become insufficient.
These are ``adversarial'' in that there is a tension between hiding systematics in the dataset and detecting these effects, and we can exploit this to improve our detection.
The systematics will be designed to be difficult to detect, and our detection method will be challenged to correct for these systematics. 
We expect to show that our approach to systematics correction addresses some of the insufficiencies of current methods, resulting in less biased measurements and the ability to detect subtle signals in the distribution.

Our testing suite will include toy models as well as catalogs modeled after target surveys, with the appropriate window function.
We will inject systematic biases in density by subsampling the galaxies based on adversarially complex functions of the metadata.
The result will be comprehensive testing suites that cover many systematic effects and range from simple combinations to extreme variation.
\textbf{These adversarial extremes will create discovery potential by showing that systematics may be obscuring physical information.}
We plan to explore observables that are motivated by current theoretical models.
For example, deviations at the largest scales could indicate primordial non-Gaussianities \citep{Dalal2008}. \KSF{Connect to anomalies section here!}

\subsection{Defeating adversarial systematics with non-parametrics}

Ideally, systematics and clustering should be modeled simultaneously. 
However, even forward modelling these individually is currently untenable. \KSF{Suchyta 2016 was cited by Rezaie 2019 wrt forward modelling systematics, dig into lit here} 
While likelihood-free inference could provide an eventual path forward, for the near-future we can develop more sophisticated methods to calibrate out systematics before performing clustering analyses.
Machine learning methods have already shown promise in systematics removal in other subfields.
Adversarial neural networks have been used to remove the jet background in particle collision data at the Large Hadron Collider \citep{Shimmin2017}.
Gaussian processes (GPs) have been used to correct for instrumental systematics in exoplanet transit surveys \citep{Gibson2012, Aigrain2016}. 
GPs lend themselves to this type of problem, as they allow for nonlinear interpolation over the data space; we propose to use GPs to remove systematics in galaxy surveys.

GPs are a Bayesian approach to modeling data; they define a prior distribution over a function space, and then use the known data to produce a posterior probability distribution over unknown values. 
In practice, GPs learn the labels of a training set and generate hyperparameters characterizing the data.
These can then be used to label previously unseen data by performing a nonlinear regression over the training data space.
In our application, we will model the galaxy density and redshift distribution as complex functions of observational metadata, as described above.
\textbf{We will train the GP on the adversarial mock catalog, and then use it predict the functions that generated the systematic effects by performing a regression over the training data space.}
We can then correct for these systematics in new surveys by assigning the galaxies weights determined by these functions.

We expect to show that our GP approach does better at recovering unbiased clustering statistics for a typical level of systematics, compared to traditional methods.
Further, we will perform this test on catalogs with extreme systematics that could be obscuring interesting clustering features; we expect traditional methods to be unable to detect these features, while our method should reveal this information.
Our systematics calibration method will improve current measurements by reducing systematic biases and open up more discovery space in galaxy clustering analyses.

Call out \textbf{Intellectual merit}.
This project will make NSF-funded cosmology projects (including
\SDSSIV, \DESI, and \LSST) more productive, more precise, and more
capable of making ground-breaking discoveries.

\section{Third theme: Searches for anomalies}

As we have emphasized, the \LCDM\ model of cosmology is incredibly successful.
But we expect it to be an approximation to the true model.
Are there already visible anomalies, or will anomalies appear soon?
The challenges are statistical: How can we find them reliably, given
non-trivial systematics and the multiple-hypotheses problems.

\HOGG{intro words here for consistency?}

\subsection{Why search for issues in the large-scale structure?}

Contemporary observational large-scale structure science---and
cosmology in general---is very focused on parameter estimation.
And rightly so!
The cosmological parameters are known now to incredible precision.
And new effects are coming visible in the near future, which will add
new parameters to the list and permit us to estimate current
parameters even more precisely.

In addition, parameter estimation is a very well-defined activity:
Parameter estimation proceeds by performing likelihood-based or
Bayesian inferences.
The best-fit (or measured or estimated) parameter values are those at
(or near) the peak in the log-likelihood function, or the (log)
probability of the data given the parameters and the fundamental
physical assumptions.
The best possible precision with which the parameters can be measured
is related to the Fisher information, which is (in turn) the second
derivative of that log-likelihood function.
The activity of parameter estimation has the beauty that it is a fully
worked out and justified procedure in classical statistics, and it
delivers results that can be judged in the framework of information
theory.
And we are really, really good at it.

Interestingly and importantly to our story,
parameter estimation can proceed \emph{whether or not the fundamental theory
is in fact a good fit to the data}.
That is, it doesn't depend on the model being \emph{good}.
It depends only on the data being precise, and the parameters showing
effects in our expections for the data (as encoded by the log-likelihood function).

But we believe that it makes sense to think about the goodness-of-fit,
or the respects in which the data do and do not fit the model.
This question is as old (in statistics) as the question of parameter
estimation, and older, even in the history of cosmology.
But it is a harder question, because it involves comparing
qualitatively different models or paradigms.

We are going to phrase these questions of goodness-of-fit in terms of
\emph{anomalies}, or respects in which the data \emph{do not conform}
to the predictions of the dominant paradigm.
We are suggesting this focus on anomalies for two connected reasons.
The first is that there have been proposed or claimed anomalies in the
cosmic microwave background (\CMB), and the conversation around them has been
productive but flawed in important ways.
Indeed none of the proposed anomalies---for example the power
asymmetry (\HOGG{CITE}), the axis of evil (\HOGG{CITE}), and the cold
spots (\HOGG{CITE})---is a confident statistical result at this point.
This is partly because these results are subtle. And partly because
they were discovered in an \foreign{ad hoc} way, which is hard to
assess in any \foreign{a posteriori} statistical analysis.
The productivity of these results is to be admired, but the statistical
context needs to be fixed. That's what we propose to do here.

The second reason for a focus on anomalies is the fact (mentioned
above) that the \LCDM\ model must sure surely be an approximation to a
more intricate theory.
Complexity isn't \emph{required}.
But it would be odd that the light sector of quarks and leptons and
force carriers would show so much phenomenology and the dark sector of
dark matter would show none.
What we know from years of \LCDM\ is that if anomalies exist, they are
hiding: There are no obviously wrong predictions of \LCDM\ known at
scales larger than 10-ish Mpc, and the issues at smaller scales are
all consistent at the present day with the interference of baryonic
physics (\HOGG{CITE}).
So that says that we need to look for subtle effects, or effects that
don't project strongly onto two-point-function measurements.

\subsection{Search for weakly broken homogeneity; hypothesis registration}

In some ways the most interesting of the \CMB\ anomalies is the
North-South power asymmetry. It's interesting because it is also
\emph{natural}: If statistical isotropy or homogeneity is weakly
broken, it will show up first as an $\ell=1$ distortion (one half of
the sky different from the other, slightly).
This motivates a search for anomalies in the \LSS\ that are small
moves away from statistical homogeneity.
To first order, in any finite survey volume, these will appear as
\emph{gradients} of statistical properties with respect to three-space
position.
These kinds of anomalies are the first for which we will search, not
because we think these anomalies are likely to appear, but because they
are natural, and they are easily enumerated.

Even in this small space of anomalies, there are subtleties. The first
is that there are many two-point statistics (say), and each of these can
show a gradient. For example, there is power at different spatial scales,
or quantities (like cosmological parameters) derived from those power
measurements. For example, we could look for a gradient in the variance at
$20\,h^{-1}\,\Mpc$ scales. Or we could look for a gradient in the location
of the baryon acoustic feature. Or in the inferred value of the amplitude
$\sigma_8$ of the power spectrum for the initial conditions.
We need to find an enumeration of the possibilities (and, preferably, an
ordering for hypothesis registration; see below).

The second subtlety is that there will be some gradients that are easier
to measure than others. There are trivial considerations, like which
quantities show high signal-to-noise in the data set. But there are less
trivial considerations, like which kinds of gradients could be mocked or
masked by systematic effects.
This links this theme to the second theme of modeling systematics:
The framework we build for modeling systematics will tell us which
kinds of anomalies we can securely detect, and at what amplitudes we
can detect them.

So even in this tiny sandbox---looking only at weakly broken
statistical homogeneity---there is quite a workflow. It looks
something like this:
\begin{itemize}
\item
Write a comprehensive list of all statistics for which we are going to search
for gradients. This must be chosen in advance.
\item
Choose a survey target. This could be
\SDSSIV\ \project{e\acronym{BOSS}}. Obtain a full set of mocks, along
with realistic systematic issues and modeled empirically with
systematics models as in the second theme.
\item
Search for the gradients in all of the statistics in the mock
data. These searches should turn up null (since the simulations are
homogeneous). This process will test all code, and give us an
opportunity to check expectations.
\item
Perform injection-recovery tests, in which we inject statistical inhomogeneities
(yes, we have ideas about how to do this by deleting and adding galaxies in a
spatially biased way) and attempt to recover injected anomalies.
This recovery needs to involve re-fitting of systematics as in the previous
theme, because some anomalies will look like systematic issues (most won't though).
We should find, in the end, that some gradient directions are better constrained than others.
\item
Pre-register the enumerated list of statistics and the data-analysis code that
searches for them, after fitting the flexible systematics model.
This pre-registration might involve a GitHub+Zenodo tag or it could be
a full arXiv publication.
\item
Operate on the data, visualize, write up and publish results of the
analysis with the pre-registered code.
\item
If any anomalies appear in that analysis, follow them up with further
investigation, but not contaminating or re-considering any of the results
of the pre-registered analyses.
\end{itemize}

The nice thing about all this is that \textbf{the ideal tool for
  finding cosmological gradients in power is the continuous
  correlation-function estimator we are building} in the first theme:
In the same way that it doesn't require bins in separation, it also
doesn't require bins in any housekeeping data. That is, the pairs
being counted in the terms can be projected not just onto a separation
basis, but also a three-space location basis in the Hubble
Volume. That is, every pair of galaxies will have not just a
separation $s$ in (say) redshift space, it will also have a position
(maybe the center-of-mass position $x$ in three-space inside the
survey.  The correlation-function basis functions in $s$ can be
multiplied by a constant function in $x$ and by three independent (orthogonal
probably) linear functions of
$x$ to make a new smooth basis $f(s,x)$. If the Universe is truly
homogeneous, statistically, the part of the basis that depends on $x$
will only be populated at a level expected by noise; all of the
significantly non-zero amplitudes should be on the part of the basis
that is constant in $x$.

The execution of this theme depends on the products of the
previous two themes.

\textbf{One challenge of this theme is the pre-registration requirement.}
The arXiv explicitly rejects papers that it regards as ``proposals''. Will a
pre-registration document or paper be regarded as a proposal? In principle
tagged GitHub code represents a pre-registration. But of course it is possible
to modify past history, so it isn't as secure as an arXiv posting.
We don't expect anyone to attempt to do anything nefarious, but if we are
going to try to be statistically principled, we probably ought to go fully principled!
This is why Zenodo might be relevant, since it can snapshot projects in a tagged
state. \HOGG{CHECK THIS.}
Solving the pre-registration problem is explicitly part of what we are proposing to do.
We don't know the best solution but we are explicitly proposing to work through possibilities
and find a solution that could be used not just by us but by the whole natural-science
community doing hard things in observational data.

\HOGG{KSF: Can you look up what Blanton and co wrote about power gradients
in the LSS and cite it?}

\subsection{Comprehensive, theory-motivated searches}

Above we said that it would be good if the anomalies for which we search are
lexicographically ordered.

....\HOGG{reference point}

\HOGG{MAKE SURE ALL THIS GETS SAID IN THIS SECTION, OR MODIFY THE SUMMARY:
The project will build methodologies that perform (and perform
ourselves) statistically principled brute-force searches of \LSS\ 
surveys (current and future) for theory-motivated departures from
predictions of the \LCDM\ model.
The cosmological model is very successful.
But the discovery of a departure at intermediate-to-large scales
(where the physical model is extremely accurate) would be so important
and significant that it is worth investing effort here.
The principled aspects of the project will include pre-registration of
hypotheses (that is, an enumeration of the specific forms and kinds of
deviations) prior to search.
And this tool category is related to the previous two, because we need
to distinguish small departures from systematic effects, and we need
to extract signatures at bound-saturating precision.}

\KSF{Adding in some theory-motivated work here}
One avenue along which we will search for anomalies is a spatial dependence of clustering.
Previous work has examined the variation of the cosmological parameters in spatial patches \citep{Mukherjee2018}.
Our approach will allow us to look systematically for spatial variations in clustering that are motivated by physical theory.
This will probe deviations at the largest scales, which could indicate unseen effects such as primordial non-Gaussianities \citep{Dalal2008}.

HOGG: Call out \textbf{Intellectual merit}.
This project will establish methods for pre-registration of hypotheses that
will be of general value in cosmology and other areas in the natural
sciences.

HOGG: Call out \textbf{Broader impacts}.

\section{Workshops and pedagogical publications}

In executing the project, the investigators will build curricular
materials aimed at undergraduates transitioning to graduate school.
Our goal is to prepare students
in the physical sciences to have better data-science and statistics
skills and therefore better PhD preparations.
Contexts for using these materials include undergraduate-to-PhD bridge programs, courses for advanced undergraduate and early graduate students, and existing research experience programs. 
We will test and refine these curricular materials in workshops for
students of broad backgrounds in New York City.
All of the materials we create will be open-source and available online. 

Many of the skills critical for a career in astrophysics are not explicitly taught in the college physics curriculum. 
These include data analysis, statistics, and data science, which are fundamental to much of modern astronomy.
Graduate students are expected to pick up these concepts, yet there is a lack of pedagogical materials for understanding them at a high level with applications to astrophysics.
To fill this gap, we will create materials that contain exercises and examples following best pedagogical practices, while also serving as reference materials for students and active researchers.

It has been shown that the largest drop in continuation of a career in astrophysics for underrepresented minorities is at the undergraduate-to-PhD level. \KSF{I think this is true, will find citation - use it to make our case about why we're focusing on this stage}

We will use these materials to teach workshops on these topics for our target audience of advanced undergraduates.
The investigators are well-versed in inquiry-based teaching practices, which are shown to increase engagement and understanding, with a focus on equity and inclusivity.
We will design the workshops using these practices, to ensure that students are building their knowledge regardless of their prior experience.
The investigators have connections with groups in New York City to hold these workshops for, through the Institute for Science and Engineer Educators. \KSF{Throwing things around here, feel free to cut...}
With feedback from these workshops, we will iterate on the materials, and finally advertise and distribute them online. 

Call out \textbf{Intellectual merit}.

Call out \textbf{Broader impacts}.

\section{Prior NSF support}

Here we list \NSF\ grants that have supported the PI within the last five years.

\paragraph{\acronym{OAC-1841594}
\project{Collaborative Research: Community Planning for Scalable Cyberinfrastructure to Support Multi-Messenger Astrophysics} (2018-2020)}
This one-year (plus extension) grant (for \$36K) is part of a multi-institutional project
to design and propose a national center for multi-messenger astrophysics. The grant supported
meetings, travel, and writing by PI Hogg and Federica Bianco (now at University of Delaware).

\paragraph{\acronym{AST-1517237}
\project{New Probabilistic Methods for Observational Cosmology}
(2015--2019)} This grant (for \$328K) supported research into Bayesian
methods for cosmology. It mainly supported research by PI Hogg and
Alex Malz (now at University of Bochum). The grant proposal was co-written
by Dr Malz and established his career.
Under this grant, Dr Malz became one of the leaders in planning cosmological
observations with the \project{Large Synoptic Survey Telescope}.
This grant supported or partially supported 24 publications
in the refereed literature and one in preparation.

\paragraph{\acronym{IIS-1124794}
\project{A Unified Probabilistic Model of Astronomical Imaging} (2011--2016)}
This grant (for \$675K) was a Cyber-Enabled Discovery Type I Grant
which supported interdisciplinary research at the intersection of
astrophysics and computer vision.  It supported research by both
Hogg's group and the group of Prof Rob Fergus in Computer Science at
NYU.
The grant also supported research by multiple graduate students in the
Hogg and Fergus groups at NYU.
The grant supported both methodological and open-source software work
on \acronym{MCMC} methods, image processing, and spectral analysis.
It supported or partially supported
more than 40 refereed publications in the astronomical and
computer-vision literatures combined.
Work on this grant established the career of Ross Fadely, who built unusual
interdisciplinary expertise during the award period and who is now the
Chief of Data Science at the \textit{Wall St Journal}.

\paragraph{Others}
In addition to the recent grants listed above, the PI's group has been supported by
\acronym{AST-0908357}
\project{Dynamical models from kinematic data: The Milky Way Disk and Halo} (2009-2011),
and
\acronym{AST-0428465}
\project{Automated Astrometry for Time-Domain and Distributed Astrophysics} (2004--2007),
and
\acronym{PHY-0101738}
\project{Theoretical Particle Physics, Astrophysics and Cosmology} (2001--2004).
and (in ancient history) a \NSF\ graduate fellowship.
The majority of the PI's 220 refereed publications have been at least partially supported
by \NSF grants at a range of different institutions..

Kate Storey-Fisher has been supported by two \NSF\ \acronym{REU} grants.
Both Storey-Fisher and Hogg have benefitted from \NSF\ support of conferences and meetings,
including the Kavli Summer Program in Astrophysics and programs at the Aspen Center for Physics.

\section{Relevance to NSF priorities and programs}

\HOGG{Can we ALSO mention the \acronym{CDS\&E} relevance in
the theme sections above?}

In addition to being relevant to \NSF\ \acronym{AAG} Program, this
project also connects strongly to the \NSF\ \acronym{CDS\&E}
program. This proposal is literally to ``promote the creation,
development, and application of the next generation of mathematical,
computational and statistical theories and tools that are essential
for addressing the challenges presented to the scientific and
engineering communities by the ever-expanding role of computational
modeling and simulation and the explosion and production of digital
experimental and observational data.'' It's embarassing to fully quote
that, but it is perfect. We also match the other \acronym{CDS\&E} criteria:
We ``create, develop, and apply novel \ldots statistical methods, algorithms, software''.
With our hypothesis registation we also ``encourage adventurous ideas that generate
new paradigms''. In short, this proposal fits perfectly with the \acronym{CDS\&E} program
criteria.

In addition, this propoal connects to several high-level \NSF\ themes:

\paragraph{Harnessing the Data Revolution:}
This proposal is explicitly motivated by changes to the data landscape
in cosmology.  As projects get larger and work to observe more volume,
not only do the data sets grow in size, they also grow in complexity
and challenge for the user.  For example, as we look further, we look
to noisier tracers, through more foregrounds, and superimposed on
stronger backgrounds. This proposal is to address these challenges
directly, and build tools that will benefit all projects facing these
challenges.  In general the challenge of making more and more precise
measurements in larger and larger data is two-fold: The data get
bigger, but the data also get harder to use responsibly as well. This
work is as motivated by these challenges as it is by the cosmological
science we are enabling.

\paragraph{Growing Convergence Research:}
The work we are doing on correlation-function estimators connects
directly to the point-process literature. We performed literature
searches, and we found almost no citations to the point-process
literature in the cosmological literature, and almost no citations to
the cosmology literature in the point-process literature. That's odd,
since cosmologists are among the biggest users of point-process
statistics in the natural sciences. We intend to address and correct
this problem and disconnect between two communities that will benefit
from one another. As subtleties in the data grow more important,
subtleties in our methodological approaches might also grow more
critical. We need interdisciplinarity at this interface.

\paragraph{Windows on the Universe:}
While this project is not explicitly multi-messenger, it supports
multi-messenger goals around the dark sector and dark matter. If we
find anomalies, they might have implications for direct (laboratory)
detection of dark matter, production of dark matter in colliders, or
annihilation. If dark matter is composed of compact objects, this work
will eventually connect to gravitational-wave astronomy. These
connections are speculative, but they could pay off immensely in the
new multi-messenger era.

\section{Not-so-frequently asked questions}

We recognize that reviewing NSF proposals can be challenging.
For this reason, we have tried to anticipate and answer some of the
questions that you, as a reviewer, may have, after reading this
proposal.

\paragraph{Why does this proposal have two authors?}

\paragraph{Isn't it ridiculous to look for anomalies in the large-scale structure?} \KSF{What kinds of anomalies would you be most likely to detect?}

\paragraph{Your clustering estimator seems so general, why are you spending so many words talking about how it relates to simulations?}

\paragraph{Don't you still have to make choices about your basis functions for your estimator, so you're not totally eliminating bining?}

\paragraph{Are you saying that the canonical two-point correlation function estimator is wrong?} \KSF{Well kind of, but that's not what this estimator is solving}

\paragraph{Aren't there already methods for dealing with nonlinear systematics?} \KSF{Yes, but they're still inadequate, and there aren't methods for dealing with combinations of systematics (I don't think...)}

\paragraph{What is the connection between systematics correction and anomaly detection?}

\paragraph{Isn't a brute-force, comprehensive search for \LSS anomalies impossible due to computational cost?}

\paragraph{Will pedagogical publications and workshops really have a broad impact?}

\paragraph{Why this team?}

\clearpage
\bibliography{description}
\bibliographystyle{apj}

\end{document}
